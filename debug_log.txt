====================嗨嗨嗨我又来了啊====================
Log_num: 1
Date: 2023.03.09
Time: 13:41
Contents:
	1). Rename 'ActorNetwork' to 'Actor', 'CriticNetwork' to 'Critic'
	2). Move files in ./simulation/PG_based/ to ./simulation/AC_based/
	3). Add basic classes 'ProbActor' and 'DualCritic' in ./common/common_cls.py
	4). Fix some bugs caused by 1), 2), and 3)
=================奥利给兄弟们干他就完了=================
	

====================嗨嗨嗨我又来了啊====================
Log_num: 2
Date: 2023.03.10
Time: 00: 17
Contents:
	1). Add SAC
	2). Add SAC-4-CartPole.py in /simulation/AC_based/
=================奥利给兄弟们干他就完了=================
	

====================嗨嗨嗨我又来了啊====================
Log_num: 3
Date: 2023.03.11
Time: 19:24
Contents:
	1). Add a new environment: CartPoleAngleOnly. 
		This new environment only takes angular information as the states of RL.
		We don't care the positional state of the cartpole.
	2). Add SAC-4-CartPoleAngleOnly.py in /simulation/AC_based/
	3). Add TD3-4-CartPoleAngleOnly.py in /simulation/AC_based/
	4). Add a well-trained controller for CartPoleAngleOnly using TD3 in /datasave/networks/TD3-CartPoleAngleOnly/parameters
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 4
Date: 2023.03.11
Time: 19:50
Contents:
	1). Discard the return variables of function 'step_update(action)' in all envs.
	2). Add different directories for different RL algorithms in /simulation/AC_based
=================奥利给兄弟们干他就完了=================

	
====================嗨嗨嗨我又来了啊====================
Log_num: 4
Date: 2023.03.11
Time: 20:57
Contents:
	1). Fix some tiny bugs.
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 5
Date: 2023.03.13
Time: 20:54
Contents:
	1). Add A2C.
	2). Add A2C-4-CartPoleDiscreteAngleOnly.py in /simulation/AC_based/A2C/
	3). Fix some bugs.
注：
    破玩意不好使，我怀疑是我写的有问题，后来从 github 上找了两个 demo，然后用 ChatGPT 也生成了一个demo，都不好使。
    A2C 用来控制 gym 那个cartpole-v1 有点作用，但是动作选择才两个，我就是盲猜都有 50% 的概率，还用它学。
    我自己写的 CartPoleDiscreteAngleOnly 环境的动作最开始是33个选择，后来13个，最后7个；角度范围从30度，缩小到20度，最后是10度，都不好使。
    相当于我把正确答案都送到它嘴边了，都尼玛学不出来。
    反观TD3，连续动作空间，经验池采20000左右，1分钟不到直接完美......还是得使用经验池啊..>_<
Tips:
    The darn thing doesn't work. I suspect there's something wrong with my code.
    Later, I found two demos on GitHub and a demo generated by ChatGPT. The three demos didn't work either.
    A2C works a bit for controlling gym's cartpole-v1, but it only has two actions.
    I'm just guessing with a 50% chance.
    I created my own CartPoleDiscreteAngleOnly environment with initially 33 action choices, then reduced to 13, and finally 7;
    The angle range went from 30 degrees to 20 degrees to 10 degrees, but none of them worked. God...
    It's as if I've handed it the correct answer on a silver platter, and it still can't learn.
    On the other hand, TD3 with a continuous action space and an experience pool of around 20,000 can learn perfectly in less than a minute...
    I guess I still need to use the replay buffer.
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 6
Date: 2023.03.14
Time: 20:54
Contents:
	1). Add a controller for CartPole with A2C.
Tips:
    This environment is different the one in gym. There are two main difference.
    First, the action of the CartPole in gym is discrete, while main is continuous.
    Then, the training goal of the CartPole in gym is just stabilizing the Pole,
        while main is stabilizing the Pole and maintaining the Cart at x=0 simultaneously.
    However, the performance is worse than just stabilizing the Pole
        because the Pole and the Cart have conflict of interest in some scenarios.
    I insist that controlling both Cart and Pole is more meaningful and challengeable
        and that is what DRL should do.
    Currently, my controller can stabilize the Pole but leave the Cart keeps fluctuating around x=0.
    目前摆能稳定住，但是小车在搁那嘎达晃晃悠悠的。(不是动力学的问题，我加了阻力，所以理论上有能量耗散，绝对可以稳住不动的)
    If anyone happens to see my code and has any idea to improve the performance, please email me.
        E-mail: yefeng.yang@connect.polyu.hk
        Respect~ 瑞思拜~
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 7
Date: 2023.03.15
Time: 19:48
Contents:
	1). Add PPO.
	2). Add a controller for CartPoleAngleOnly using PPO.
	3). Add a controller for FlightAttitudeSimulator2StateContinuous using PPO.
Tips:
    The performance is pretty good.
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 7
Date: 2023.03.16
Time: 11:28
Contents:
	1). Add PPO-4-CartPole.py
	2). PPO-4-UGVBidirectional.py
Tips:
    No well-trained controller for the two envs yet.
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 8
Date: 2023.03.21
Time: 20:26
Contents:
	1). Add DPPO
	2). Add DPPO-4-CartPoleAngleOnly.py
	3). Add a controller for CartPoleAngleOnly using DPPO.
	4). Add DPPO-4-FlightSimulator2State.py
	5). Add a controller for FlightSimulator2State using DPPO.
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 9
Date: 2023.03.23
Time: 21:38
Contents:
	1). Add DPPO2 (without debugging)
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 10
Date: 2023.03.28
Time: 22:43
Contents:
	Nothing, just remind that, DPPO2 doesn't work!!

Tips: 疯了，一周白干，DPPO2 不好使，不知道是哪里的代码出问题了。
    DPPO2 使用了 python 较新的 shared_memory 作为通信机制，这样做快，我对比了一个 github上 用 pipe 的，真的是快的特别多，
    但是这会和 CUDA 的多进程冲突，所以只能用 CPU。但是当我只开一个进程的时候，训练完全没有效果，而同样的环境，用 PPO 秒收敛。
    用 DPPO1，因为我减小了学习率，前期因为 agent 太多，收敛也不容易，所以 DPPO1 收敛地比 PPO 慢很多，但是很稳，最后效果相同。
    所以，肯定不是 DPPO2 网络训练问题，应该是中间某个环节的数据传输出问题了 (多进程通信，包括探索的数据通信，网络参数通信，等等)。
    github 上没有找到用 shared_memory 写的 (虽然我坚持认为这么做最科学，但是 mp.shared_memory 好像是 python3.8 之后的，所以可能暂时没有这么写的)。
    debug 到吐，最后也没查出来是谁的问题。。。
    哪位大佬碰巧看到，可以浏览一下 /algorithm/policy_base/Distributed_PPO2.py 和 /simulation/POlicy_based/DPPO2-4-CartPoleAngleOnly.py 这俩是对应的。
    多谢大佬...... >_< Q_Q T_T
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 10
Date: 2023.03.29
Time: 22:15
Contents:
    1). 更新了所有算法与环境交互的方式：删除了模型描述文件 (文件本身还在，只是不再使用)
    2). 算法内部加载 env 环境，可以内部交互
    3). 修改了DQN、DuelingDQN 和 DoubleDQN 的部分结构
    4). 修改了部分文件的路径
    5). 重写了 README.md

Tips：
    至此，开始逐步将仓库 ReinforcementLearning 修改成 ReinforcementLearning_V2，这也意味着 ReinforcementLearning 将停止更新。
    ReinforcementLearning_V2 所有的功能与用法均与 ReinforcementLearning 完全相同，之前训练好的控制器也能直接在 V2 上运行 (如果没有bug)。
    当 V2 的调试完全结束之后，ReinforcementLearning_V2 会被重新命名为 ReinforcementLearning 以覆盖目前的版本。
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 10
Date: 2023.03.30
Time: 22:59
Contents:
    1). Add new env: UGV2.UGVBidirectional
    2). Add new env: SecondOrderIntegrationSystem
=================奥利给兄弟们干他就完了=================


====================嗨嗨嗨我又来了啊====================
Log_num: 10
Date: 2023.04.03
Time: 22:59
Contents:
    1). Add new controller: a positional controller for SecondOrderIntegration using DPPO.

Tips:
    需要网络重训练的方法，在第一次保存的网络中，挑一个比较好的，以此为基础，再训练第二波，以此类推，直到有好的为止。
    奖励函数分为两部分，位置奖励 和 角度奖励。
    位置奖励：负的误差的绝对值，在加一个正的偏置 (跟二次型效果相同)
    角度奖励：误差向量与速度向量夹角的奖励，这个是为了增加轨迹的直线性。奖励数值为 -(夹角 - 45度) * gain，夹角小于45度的时候是就开始给正奖励了。
    确实学出来了，但是用的是DPPO，这就意味着同样的参数，用 PPO 或者 SAC，TD3 可能就不太好学，要么因为数据太少学不完，要么因为走的步子太大震荡。
    拼算力啊......我这电脑太垃圾了，以后有机会弄个 i9 处理器，核再多点能更好 (我的电脑目前也就5个进程，想跑10个也行，但是速度跟5个区别不太大了) 再或者直接上ISSAC Gym......
    奥利给干了
=================奥利给兄弟们干他就完了=================


Log_num: 11
Date: 2023.04.12
Time: 22:29
Contents:
    1). Add discrete version of PPO: PPO_Discrete
    1). Add new controller: a positional controller for SecondOrderIntegration-BangBang using PPO_Discrete.

Tips:
    
=================奥利给兄弟们干他就完了=================